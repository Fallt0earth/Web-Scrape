{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ad6d370",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load done\n"
     ]
    }
   ],
   "source": [
    "import urllib\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import pandas as pd\n",
    "import json\n",
    "import spacy\n",
    "import sklearn as sk\n",
    "\n",
    "path=\"C:\\\\Users\\\\dstok\\\\Documents\\\\web scraper\\\\archive\\\\companies_sorted.csv\"\n",
    "\n",
    "df = pd.read_csv(path)\n",
    "print(\"load done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3cb9ef2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(52000, 40273)\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.en import English\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "df = df.dropna()\n",
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "nlp = English()\n",
    "df = df[0:52000]\n",
    "df = df.dropna()\n",
    "token_list = []\n",
    "i=0\n",
    "vectorizer = CountVectorizer()\n",
    "for row in df['name']:\n",
    "    text = nlp(row)\n",
    "    token_list.append(text.text)\n",
    "        \n",
    "    #if i == 10000:\n",
    "        #print(i)\n",
    "        #break\n",
    "    #i+=1\n",
    "#print(token_list)\n",
    "vectorizer = CountVectorizer()\n",
    "bow_vector = vectorizer.fit_transform(token_list)\n",
    "#print(bow_vector)\n",
    "print(bow_vector.shape)\n",
    "#df['vector'] = bow_vector\n",
    "# Create list of word tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9b71c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22af5983",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(52000, 11)\n",
      "['Unnamed: 0' 'name' 'domain' 'year founded' 'industry' 'size range'\n",
      " 'locality' 'country' 'linkedin url' 'current employee estimate'\n",
      " 'total employee estimate']\n",
      "(52000, 40273)\n",
      "(52000,)\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)\n",
    "print(df.columns.values)\n",
    "\n",
    "#df.drop['name']\n",
    "\n",
    "X = bow_vector\n",
    "Y = df['industry']\n",
    "\n",
    "print(X.shape)\n",
    "print(Y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b305600",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running\n",
      "Accuracy: 0.36023076923076924\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(X,Y)\n",
    "#print(x_train)\n",
    "#print(df.shape)\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_classification\n",
    "X, y = make_classification(n_samples=52000, random_state=0)\n",
    "clf = make_pipeline( LinearSVC(random_state=0, tol=1e-5))\n",
    "clf.fit(x_train, y_train)\n",
    "                    \n",
    "print(\"running\")\n",
    "test_score = clf.score(x_test, y_test)\n",
    "print(\"Accuracy:\", test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c25e6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "914f1b17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "25 25 25 25 25\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "import urllib\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import pandas as pd\n",
    "import json\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#https://hackernoon.com/how-to-scrape-google-with-python-bo7d2tal\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "API = 'IHOD7nIduQHdIIogy8on'\n",
    "URL = \"https://api.opencorporates.com/v0.4/companies/search?q=Bank&jurisdiction_code=us_fl\" + \"?api_token=\" + API\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "name = []\n",
    "profit = []\n",
    "incorp = []\n",
    "classification = []\n",
    "addy = []\n",
    "for i in range(0, 25):\n",
    "    URL = \"https://api.opencorporates.com/v0.4/companies/us_fl/N\" + str(19939 + i)  + \"?api_token=\" + API\n",
    "\n",
    "    resp = requests.get(URL)\n",
    "    #DO NOT REMOVE\n",
    "    time.sleep(.2)\n",
    "    print(resp)\n",
    "    if resp.status_code == 200:\n",
    "\n",
    "        data = json.loads(resp.content)\n",
    "        pretty = json.dumps(data, indent=2)\n",
    "        results = data['results']\n",
    "        name.append(results['company']['name'])\n",
    "        profit.append(results['company']['company_type'])\n",
    "        incorp.append(results['company']['incorporation_date'])\n",
    "        addy.append(results['company']['data']['most_recent'][0]['datum']['description'])\n",
    "        test = name\n",
    "        doc = nlp(name[i])\n",
    "        classification.append(doc.text)\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "\n",
    "#https://github.com/DivakarPM/NLP/blob/master/Text_Summarization/Text_Summarization.ipynb\n",
    "        \n",
    "print(str(len(name)) + \" \" + str(len(incorp)) + \" \" + str(len(classification)) + \" \" + str(len(profit)) + \" \" + str(len(addy)))\n",
    "\n",
    "vectors = vectorizer.transform(classification)\n",
    "arr = clf.predict(vectors)\n",
    "hashed = {'Name' : name, \"incorporated\" : incorp, \"Business type\" : profit, \"address\" : addy, \"classification\" : arr}\n",
    "print(\"done\")\n",
    "df = pd.DataFrame.from_dict(hashed)\n",
    "df.dropna(inplace=True)\n",
    "df.to_csv(\".\\out.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6339ea4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
